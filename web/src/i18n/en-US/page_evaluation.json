{
  "metadata": {
    "title": "Evaluation",
    "description": "Efficiently track, manage, and review the historical performance of your Retrieval-Augmented Generation (RAG) evaluations all in one place."
  },
  "search": "Search",
  "add_evaluation": "Add evaluation",
  "delete_evaluation": "Delete evaluation",
  "delete_evaluation_confirm": "This action cannot be undone. This will permanently delete evaluation and remove your evaluation from our servers.",
  "retry_all_evaluation": "Retry All",
  "retry_all_evaluation_confirm": "Are you sure you want to re-evaluate all questions?",
  "retry_failed_evaluation": "Retry Failed",
  "retry_failed_evaluation_confirm": "Are you sure you want to re-evaluate these failed questions?",

  "questions": "Question",
  "avg_score": "Avg. Score",

  "evaluation_name": "Evaluation name",
  "evaluation_name_placeholder": "Enter the evaluation name",

  "collection": "Collection",
  "collection_placeholder": "Select a collection",
  "add_collection": "Add Collection",

  "question_set": "Question Set",
  "question_set_placeholder": "Select a question set",
  "add_question_set": "Add Question Set",

  "agent_llm": "Agent LLM",
  "agent_llm_placeholder": "Select a agent model",

  "judge_llm": "Judge LLM",
  "judge_llm_placeholder": "Select a judge model",

  "ground_truth": "Ground Truth",
  "rag_answer": "RAG Answer",
  "judge_reason": "LLM Judge Reasoning"
}
